% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adapdiscom.R
\name{adapdiscom}
\alias{adapdiscom}
\title{AdapDiscom: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors}
\usage{
adapdiscom(
  beta,
  x,
  y,
  x.tuning,
  y.tuning,
  x.test,
  y.test,
  nlambda,
  nalpha,
  pp,
  robust = 0,
  standardize = TRUE,
  itcp = TRUE,
  lambda.min.ratio = NULL,
  k.value = 1.5
)
}
\arguments{
\item{beta}{Vector, true beta coefficients (optional)}

\item{x}{Matrix, training data}

\item{y}{Vector, training response}

\item{x.tuning}{Matrix, tuning data}

\item{y.tuning}{Vector, tuning response}

\item{x.test}{Matrix, test data}

\item{y.test}{Vector, test response}

\item{nlambda}{Integer, number of lambda values}

\item{nalpha}{Integer, number of alpha values}

\item{pp}{Vector, block sizes}

\item{robust}{Integer, 0 for classical, 1 for robust estimation of covariance}

\item{standardize}{Logical, whether to standardize covariates. When TRUE, uses training data mean and standard deviation to standardize tuning and test sets. When robust=1, uses Huber-robust standard deviation estimates}

\item{itcp}{Logical, whether to include intercept}

\item{lambda.min.ratio}{Numeric, `lambda.min.ratio` sets the smallest lambda value in the grid, expressed as a fraction of `lambda.max`â€”the smallest lambda for which all coefficients are zero. By default, it is `0.0001` when the number of observations (`nobs`) exceeds the number of variables (`nvars`), and `0.01` when `nobs < nvars`. Using a very small value in the latter case can lead to overfitting.}

\item{k.value}{Numeric, tuning parameter for robust estimation}
}
\value{
List with estimation results
}
\description{
AdapDiscom: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors
}
\section{Value}{

The function returns a list containing the following components:

\describe{
  \item{err}{A multi-dimensional array storing the mean squared error (MSE) for all combinations of tuning parameters alpha and lambda.}
  \item{est.error}{The estimation error, calculated as the Euclidean distance between the estimated beta coefficients and the true beta (if provided).}
  \item{lambda}{The optimal lambda value chosen via cross-validation on the tuning set.}
  \item{alpha}{A vector of the optimal alpha values, also selected on the tuning set.}
  \item{train.error}{The mean squared error on the tuning set for the optimal parameter combination.}
  \item{test.error}{The mean squared error on the test set for the final, optimal model.}
  \item{y.pred}{The predicted values for the observations in the test set.}
  \item{R2}{The R-squared value, which measures the proportion of variance explained by the model on the test set.}
  \item{a0}{The intercept of the final model.}
  \item{a1}{The vector of estimated beta coefficients for the final model.}
  \item{select}{The number of non-zero coefficients, representing the number of selected variables.}
  \item{xtx}{The final regularized covariance matrix used to fit the optimal model.}
  \item{fpr}{The False Positive Rate (FPR) if the true beta is provided. It measures the proportion of irrelevant variables incorrectly selected.}
  \item{fnr}{The False Negative Rate (FNR) if the true beta is provided. It measures the proportion of relevant variables incorrectly excluded.}
  \item{lambda.all}{The complete vector of all lambda values tested during cross-validation.}
  \item{beta.cov.lambda.max}{The estimated beta coefficients using the maximum lambda value.}
  \item{time}{The total execution time of the function in seconds.}
}
}

\examples{
\dontrun{
# Generate synthetic data with block structure
set.seed(123)
n <- 100
pp <- c(20, 30)  # Two blocks
p <- sum(pp)
beta_true <- c(rep(0.5, 10), rep(0, 10), rep(0.3, 10), rep(0, 20))

# Generate covariance matrix
Sigma <- generate.cov(p = p, example = 1)

# Simulate data with missing blocks
x_full <- MASS::mvrnorm(n = n, mu = rep(0, p), Sigma = Sigma)
# Introduce block-wise missing pattern
missing_pattern <- sample(1:2, n, replace = TRUE, prob = c(0.3, 0.7))
x <- x_full
x[missing_pattern == 1, (pp[1] + 1):p] <- NA  # Missing second block

# Generate response
y <- x_full \%*\% beta_true + rnorm(n, 0, 0.1)

# Split data
train_idx <- 1:60
tune_idx <- 61:80
test_idx <- 81:100

result <- adapdiscom(
  beta = beta_true,
  x = x[train_idx, ],
  y = y[train_idx],
  x.tuning = x[tune_idx, ],
  y.tuning = y[tune_idx],
  x.test = x[test_idx, ],
  y.test = y[test_idx],
  nlambda = 10,
  nalpha = 5,
  pp = pp
)

# View results
print(paste("Test error:", round(result$test.error, 4)))
print(paste("R-squared:", round(result$R2, 4)))
print(paste("Variables selected:", result$select))
}
}
