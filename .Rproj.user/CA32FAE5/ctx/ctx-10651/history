x = NULL
is.null(x)
?glmnet::glmnet
roxygen2::roxygenize()
roxygen2::roxygenize()
roxygen2::roxygenize()
roxygen2::vignette_roclet()
library(roxygen2)
roxygenize()
roxygenize()
library(AdapDiscom)
library(MASS)
# Set parameters
n <- 100        # Training sample size
n.tuning <- 50  # Tuning sample size
n.test <- 50    # Test sample size
p <- 50         # Number of variables
pp <- c(20, 15, 15)  # Block sizes (must sum to p)
# Generate block-diagonal covariance matrix
Sigma <- generate.cov(p, example = 2)  # Block diagonal structure
# Generate true beta coefficients
beta.true <- c(rep(2, 10), rep(0, 10), rep(1, 10), rep(0, 5), rep(-1, 10), rep(0, 5))
# Generate training data
X.train <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
y.train <- X.train %*% beta.true + rnorm(n)
# Generate tuning data
X.tuning <- mvrnorm(n.tuning, mu = rep(0, p), Sigma = Sigma)
y.tuning <- X.tuning %*% beta.true + rnorm(n.tuning)
# Generate test data
X.test <- mvrnorm(n.test, mu = rep(0, p), Sigma = Sigma)
y.test <- X.test %*% beta.true + rnorm(n.test)
X.train
library(AdapDiscom)
library(MASS)
# Set parameters
n <- 100        # Training sample size
n.tuning <- 50  # Tuning sample size
n.test <- 50    # Test sample size
p <- 50         # Number of variables
pp <- c(20, 15, 15)  # Block sizes (must sum to p)
# Generate block-diagonal covariance matrix
Sigma <- generate.cov(p, example = 2)  # Block diagonal structure
# Generate true beta coefficients
beta.true <- c(rep(2, 10), rep(0, 10), rep(1, 10), rep(0, 5), rep(-1, 10), rep(0, 5))
# Generate training data
X.train <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
y.train <- X.train %*% beta.true + rnorm(n)
n1=n2=n3=n4=n%/%4
p1 = pp[1]
p2 = pp[2]
p3 = pp[3]
X.train[(n1 + 1):(n1 + n2), (p1 + p2 + 1):(p1 + p2 + p3)] <- NA
X.train[(n1 + n2 + 1):(n1 + n2 + n3), (p1 + 1):(p1 + p2)] <- NA
X.train[(n1 + n2 + n3 + 1):(n1 + n2 + n3 + n4), (1:p1)] <- NA
# Generate tuning data
X.tuning <- mvrnorm(n.tuning, mu = rep(0, p), Sigma = Sigma)
y.tuning <- X.tuning %*% beta.true + rnorm(n.tuning)
# Generate test data
X.test <- mvrnorm(n.test, mu = rep(0, p), Sigma = Sigma)
y.test <- X.test %*% beta.true + rnorm(n.test)
X.train
View(X.train)
library(AdapDiscom)
library(MASS)
# Set parameters
n <- 100        # Training sample size
n.tuning <- 50  # Tuning sample size
n.test <- 50    # Test sample size
p <- 50         # Number of variables
pp <- c(20, 15, 15)  # Block sizes (must sum to p)
# Generate block-diagonal covariance matrix
Sigma <- generate.cov(p, example = 2)  # Block diagonal structure
# Generate true beta coefficients
beta.true <- c(rep(2, 10), rep(0, 10), rep(1, 10), rep(0, 5), rep(-1, 10), rep(0, 5))
# Generate training data
X.train <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
y.train <- X.train %*% beta.true + rnorm(n)
n1=n2=n3=n4=n%/%4
p1 = pp[1]
p2 = pp[2]
p3 = pp[3]
X.train[(n1 + 1):(n1 + n2), (p1 + p2 + 1):(p1 + p2 + p3)] <- NA
X.train[(n1 + n2 + 1):(n1 + n2 + n3), (p1 + 1):(p1 + p2)] <- NA
X.train[(n1 + n2 + n3 + 1):(n1 + n2 + n3 + n4), (1:p1)] <- NA
# Generate tuning data
X.tuning <- mvrnorm(n.tuning, mu = rep(0, p), Sigma = Sigma)
y.tuning <- X.tuning %*% beta.true + rnorm(n.tuning)
# Generate test data
X.test <- mvrnorm(n.test, mu = rep(0, p), Sigma = Sigma)
y.test <- X.test %*% beta.true + rnorm(n.test)
# Run AdapDiscom with default parameters
result <- adapdiscom(
beta = beta.true,      # True coefficients (optional, for evaluation)
x = X.train,          # Training data
y = y.train,          # Training response
x.tuning = X.tuning,  # Tuning data
y.tuning = y.tuning,  # Tuning response
x.test = X.test,      # Test data
y.test = y.test,      # Test response
nlambda = 20,         # Number of lambda values
nalpha = 10,          # Number of alpha values
pp = pp,              # Block sizes
robust = 0,           # Classical estimation
standardize = TRUE,   # Standardize data
itcp = TRUE          # Include intercept
)
#'   \item{a1}{The vector of estimated beta coefficients for the final model.}
#'   \item{select}{The number of non-zero coefficients, representing the number of selected variables.}
#'   \item{xtx}{The final regularized covariance matrix used to fit the optimal model.}
#'   \item{fpr}{The False Positive Rate (FPR) if the true beta is provided. It measures the proportion of irrelevant variables incorrectly selected.}
#'   \item{fnr}{The False Negative Rate (FNR) if the true beta is provided. It measures the proportion of relevant variables incorrectly excluded.}
#'   \item{lambda.all}{The complete vector of all lambda values tested during cross-validation.}
#'   \item{beta.cov.lambda.max}{The estimated beta coefficients using the maximum lambda value.}
#'   \item{time}{The total execution time of the function in seconds.}
#' }
#' @export
adapdiscom <- function(beta, x, y, x.tuning, y.tuning, x.test, y.test, nlambda, nalpha, pp,
robust = 0, standardize = TRUE, itcp = TRUE, lambda.min.ratio = NULL,
k.value = 1.5) {
x_std = standardize_x(as.matrix(x), robust = robust, k.value = k.value)
xm <- x_std$x.mean
xs <- x_std$x.sd
if (standardize) {
x <- x_std$x
x.tuning <- fit_standardize_x(x.tuning, xm, xs)
x.test <- fit_standardize_x(x.test, xm, xs)
}
start_time <- proc.time()
n <- dim(x)[1]
p <- dim(x)[2]
n.tuning <- dim(x.tuning)[1]
n.test <- dim(x.test)[1]
n_blocks <- length(pp)
alpha.all <- 10^seq(10^(-10), -3, length = nalpha)
lmax <- lambda_max(x, y, Methode = "discom", robust = robust)
nobs <- dim(na.omit(x))[1]
if(lambda.min.ratio <=0|lambda.min.ratio >1|is.null(lambda.min.ratio)){
lambda.min.ratio <- ifelse(nobs < p, 0.01, 1e-04)
}
lmin <- lmax * lambda.min.ratio
lambda.all <- exp(seq(log(lmax), log(lmin), length.out = nlambda))
# Création d'un tableau multidimensionnel pour les erreurs
dim_array <- rep(nalpha, n_blocks + 1)
dim_array <- c(dim_array, nlambda)
DISCOM.tun.error <- array(NA, dim = dim_array)
xtx.raw <- compute.xtx(x, robust = robust, k_value = k.value)
xty <- compute.xty(x, y, robust = robust, k_value = k.value)
# Création des indices pour les sous-matrices
indices <- get_block_indices(pp)
# Création des sous-matrices individuelles
xtx.raw.blocks <- list()
shrink.targets <- numeric(n_blocks)
for (i in 1:n_blocks) {
idx_range <- indices$starts[i]:indices$ends[i]
xtx.raw.blocks[[i]] <- xtx.raw[idx_range, idx_range]
shrink.targets[i] <- sum(diag(xtx.raw.blocks[[i]]))/p
}
# Création de la matrice bloc-diagonale initiale
xtx.raw.I <- as.matrix(do.call(Matrix::bdiag, xtx.raw.blocks))
# Calcul de xtx.raw.C
xtx.raw.C <- xtx.raw - xtx.raw.I
# Calcul du shrink.target global
shrink.target <- sum(diag(xtx.raw))/p
# Créer des indices pour les boucles imbriquées
alpha_indices <- expand.grid(replicate(n_blocks + 1, 1:nalpha, simplify = FALSE))
# Boucle sur toutes les combinaisons d'indices alpha
for (row in 1:nrow(alpha_indices)) {
# Extraire les valeurs alpha actuelles
current_alphas <- alpha.all[as.numeric(alpha_indices[row, ])]
# Les n_blocks premières alphas sont pour les blocs diagonaux
block_alphas <- current_alphas[1:n_blocks]
# Le dernier alpha est pour xtx.raw.C
alpha_non_diag <- current_alphas[n_blocks + 1]
# Calcul du shrink.target pondéré
weighted_targets <- 0
weight_sum <- 0
for (i in 1:n_blocks) {
weighted_targets <- weighted_targets + ((1-block_alphas[i])^2) * shrink.targets[i]
weight_sum <- weight_sum + (1-block_alphas[i])^2
}
shrink.target_weighted <- weighted_targets / weight_sum
# Construction de la matrice bloc-diagonale avec les alphas appliqués
scaled_blocks <- list()
for (i in 1:n_blocks) {
scaled_blocks[[i]] <- block_alphas[i] * xtx.raw.blocks[[i]]
}
xtx.raw.I_scaled <- as.matrix(do.call(Matrix::bdiag, scaled_blocks))
# Construction de la matrice finale
xtx <- xtx.raw.I_scaled + alpha_non_diag * xtx.raw.C +
(n_blocks - sum(block_alphas)) * shrink.target_weighted * diag(p)
# Vérification de la positivité
if (min(eigen(xtx, only.values = TRUE)$values) < 0) {
# Remplir avec 10^8 pour cette combinaison d'alphas et tous les lambdas
idx <- as.list(as.numeric(alpha_indices[row, ]))
idx_str <- paste(paste(idx, collapse = ","), ",", sep = "")
eval(parse(text = paste("DISCOM.tun.error[", idx_str, "] = 10^8", sep = "")))
} else {
# Calcul des erreurs pour chaque lambda
beta.initial <- rep(0, p)
for (k in 1:nlambda) {
beta.cov <- as.vector(scout::crossProdLasso(xtx, xty, lambda.all[k],
beta.init = beta.initial)$beta)
beta.initial <- beta.cov
intercept <- ifelse(itcp, mean(y) - sum(beta.cov * (xm/xs)), 0)
DISCOM.tun.values <- as.vector(as.matrix(x.tuning) %*% beta.cov) + intercept
# Indexer correctement le tableau multidimensionnel
idx <- c(as.numeric(alpha_indices[row, ]), k)
idx_str <- paste(paste(idx, collapse = ","), sep = "")
eval(parse(text = paste("DISCOM.tun.error[", idx_str, "] = mean((y.tuning-DISCOM.tun.values)^2)", sep = "")))
}
}
}
# Trouver les paramètres optimaux
opt.index <- as.vector(which(DISCOM.tun.error == min(DISCOM.tun.error), arr.ind = TRUE)[1, ])
train.error <- min(DISCOM.tun.error)
# Extraire les valeurs alpha optimales
opt.alphas <- alpha.all[opt.index[1:(n_blocks+1)]]
block_alphas_opt <- opt.alphas[1:n_blocks]
alpha_non_diag_opt <- opt.alphas[n_blocks+1]
# Extraire le lambda optimal
opt.lambda <- lambda.all[opt.index[n_blocks+2]]
# Recalcul du shrink.target pondéré avec les alphas optimaux
weighted_targets <- 0
weight_sum <- 0
for (i in 1:n_blocks) {
weighted_targets <- weighted_targets + ((1-block_alphas_opt[i])^2) * shrink.targets[i]
weight_sum <- weight_sum + (1-block_alphas_opt[i])^2
}
shrink.target_opt <- weighted_targets / weight_sum
# Construction de la matrice bloc-diagonale optimale
scaled_blocks_opt <- list()
for (i in 1:n_blocks) {
scaled_blocks_opt[[i]] <- block_alphas_opt[i] * xtx.raw.blocks[[i]]
}
xtx.raw.I_opt <- as.matrix(do.call(Matrix::bdiag, scaled_blocks_opt))
# Construction de la matrice finale optimale
xtx_opt <- xtx.raw.I_opt + alpha_non_diag_opt * xtx.raw.C +
(n_blocks - sum(block_alphas_opt)) * shrink.target_opt * diag(p)
# Calcul du beta optimal
beta.cov <- as.vector(scout::crossProdLasso(xtx_opt, xty, opt.lambda)$beta)
beta.cov.lambda.max <- as.vector(scout::crossProdLasso(xtx_opt, xty, lmax)$beta)
intercept <- ifelse(itcp, mean(y) - sum(beta.cov * (xm/xs)), 0)
# Prédiction sur l'ensemble de test
predict.test.values <- as.vector(x.test %*% beta.cov) + intercept
# Calcul des métriques de performance
DISCOM.test.error <- mean((y.test - predict.test.values)^2)
select <- sum(as.vector(as.integer(beta.cov != 0)))
if (!is.null(beta)) {
DISCOM.fpr <- sum((beta == 0) & (beta.cov != 0))/sum(beta == 0)
DISCOM.fnr <- sum((beta != 0) & (beta.cov == 0))/sum(beta != 0)
DISCOM.est.error <- sqrt(sum((beta.cov - beta)^2))
} else {
DISCOM.fpr <- NA
DISCOM.fnr <- NA
DISCOM.est.error <- NA
}
R2 <- cor(predict.test.values, y.test)^2
end_time <- proc.time()
time_taken <- end_time - start_time
# Préparation du résultat
a <- list(
'err' = DISCOM.tun.error,
'est.error' = DISCOM.est.error,
'lambda' = opt.lambda,
'alpha' = c(block_alphas_opt, alpha_non_diag_opt),
'train.error' = train.error,
'test.error' = DISCOM.test.error,
'y.pred' = predict.test.values,
'R2' = R2,
'a0' = intercept,
'a1' = beta.cov,
'select' = select,
'xtx' = xtx_opt,
'fpr' = DISCOM.fpr,
'lambda.all' = lambda.all,
'beta.cov.lambda.max' = beta.cov.lambda.max,
'fnr' = DISCOM.fnr,
"time" = as.numeric(time_taken[3])
)
return(a)
}
adapdiscom
beta.true
# Run AdapDiscom with default parameters
result <- adapdiscom(
beta = beta.true,      # True coefficients (optional, for evaluation)
x = X.train,          # Training data
y = y.train,          # Training response
x.tuning = X.tuning,  # Tuning data
y.tuning = y.tuning,  # Tuning response
x.test = X.test,      # Test data
y.test = y.test,      # Test response
nlambda = 20,         # Number of lambda values
nalpha = 10,          # Number of alpha values
pp = pp,              # Block sizes
robust = 0,           # Classical estimation
standardize = TRUE,   # Standardize data
itcp = TRUE          # Include intercept
)
source("~/Desktop/AdapDiscom/AdapDiscomPackage/R/utility_functions.R", echo=TRUE)
# Run AdapDiscom with default parameters
result <- adapdiscom(
beta = beta.true,      # True coefficients (optional, for evaluation)
x = X.train,          # Training data
y = y.train,          # Training response
x.tuning = X.tuning,  # Tuning data
y.tuning = y.tuning,  # Tuning response
x.test = X.test,      # Test data
y.test = y.test,      # Test response
nlambda = 20,         # Number of lambda values
nalpha = 10,          # Number of alpha values
pp = pp,              # Block sizes
robust = 0,           # Classical estimation
standardize = TRUE,   # Standardize data
itcp = TRUE          # Include intercept
)
source("~/Desktop/AdapDiscom/AdapDiscomPackage/R/adapdiscom.R", echo=TRUE)
# Run AdapDiscom with default parameters
result <- adapdiscom(
beta = beta.true,      # True coefficients (optional, for evaluation)
x = X.train,          # Training data
y = y.train,          # Training response
x.tuning = X.tuning,  # Tuning data
y.tuning = y.tuning,  # Tuning response
x.test = X.test,      # Test data
y.test = y.test,      # Test response
nlambda = 20,         # Number of lambda values
nalpha = 10,          # Number of alpha values
pp = pp,              # Block sizes
robust = 0,           # Classical estimation
standardize = TRUE,   # Standardize data
itcp = TRUE          # Include intercept
)
# View results
print(result$MSE_test)
View(result)
result[["R2"]]
# Optimal parameters
cat("Optimal lambda:", result$lambda, "\n")
cat("Optimal alpha:", result$alpha, "\n")
# Performance metrics
cat("Training error:", result$train.error, "\n")
cat("Test error:", result$test.error, "\n")
cat("R-squared:", result$R2, "\n")
# Model selection
cat("Number of selected variables:", result$select, "\n")
# If true beta provided, evaluation metrics
if (!is.null(beta.true)) {
cat("Estimation error:", result$est.error, "\n")
cat("False Positive Rate:", result$fpr, "\n")
cat("False Negative Rate:", result$fnr, "\n")
}
# Estimated coefficients
head(result$a1)  # First 6 coefficients
