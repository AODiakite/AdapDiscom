library(AdapDiscom)
library(AdapDiscom)
library(MASS)
# Set parameters
n <- 100        # Training sample size
n.tuning <- 50  # Tuning sample size
n.test <- 50    # Test sample size
p <- 50         # Number of variables
pp <- c(20, 15, 15)  # Block sizes (must sum to p)
# Generate block-diagonal covariance matrix
Sigma <- generate.cov(p, example = 2)  # Block diagonal structure
# Generate true beta coefficients
beta.true <- c(rep(2, 10), rep(0, 10), rep(1, 10), rep(0, 5), rep(-1, 10), rep(0, 5))
# Generate training data
X.train <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
y.train <- X.train %*% beta.true + rnorm(n)
n1=n2=n3=n4=n%/%4
p1 = pp[1]
p2 = pp[2]
p3 = pp[3]
X.train[(n1 + 1):(n1 + n2), (p1 + p2 + 1):(p1 + p2 + p3)] <- NA
X.train[(n1 + n2 + 1):(n1 + n2 + n3), (p1 + 1):(p1 + p2)] <- NA
X.train[(n1 + n2 + n3 + 1):(n1 + n2 + n3 + n4), (1:p1)] <- NA
# Generate tuning data
X.tuning <- mvrnorm(n.tuning, mu = rep(0, p), Sigma = Sigma)
y.tuning <- X.tuning %*% beta.true + rnorm(n.tuning)
# Generate test data
X.test <- mvrnorm(n.test, mu = rep(0, p), Sigma = Sigma)
y.test <- X.test %*% beta.true + rnorm(n.test)
# Run AdapDiscom with default parameters
result <- adapdiscom(
beta = beta.true,      # True coefficients (optional, for evaluation)
x = X.train,          # Training data
y = y.train,          # Training response
x.tuning = X.tuning,  # Tuning data
y.tuning = y.tuning,  # Tuning response
x.test = X.test,      # Test data
y.test = y.test,      # Test response
nlambda = 20,         # Number of lambda values
nalpha = 10,          # Number of alpha values
pp = pp,              # Block sizes
robust = 0,           # Classical estimation
standardize = TRUE,   # Standardize data
itcp = TRUE          # Include intercept
)
# View results
print(result$R2)
# Optimal parameters
cat("Optimal lambda:", result$lambda, "\n")
cat("Optimal alpha:", result$alpha, "\n")
# Performance metrics
cat("Training error:", result$train.error, "\n")
cat("Test error:", result$test.error, "\n")
cat("R-squared:", result$R2, "\n")
# Model selection
cat("Number of selected variables:", result$select, "\n")
# If true beta provided, evaluation metrics
if (!is.null(beta.true)) {
cat("Estimation error:", result$est.error, "\n")
cat("False Positive Rate:", result$fpr, "\n")
cat("False Negative Rate:", result$fnr, "\n")
}
# Estimated coefficients
head(result$a1)  # First 6 coefficients
# Compare different methods
methods <- c("adapdiscom", "discom", "fast_adapdiscom", "fast_discom")
results <- list()
# AdapDiscom
results$adapdiscom <- adapdiscom(
beta = beta.true, x = X.train, y = y.train,
x.tuning = X.tuning, y.tuning = y.tuning,
x.test = X.test, y.test = y.test,
nlambda = 20, nalpha = 10, pp = pp
)
# DISCOM
results$discom <- discom(
beta = beta.true, x = X.train, y = y.train,
x.tuning = X.tuning, y.tuning = y.tuning,
x.test = X.test, y.test = y.test,
nlambda = 20, nalpha = 10, pp = pp
)
# Fast AdapDiscom
results$fast_adapdiscom <- fast_adapdiscom(
beta = beta.true, x = X.train, y = y.train,
x.tuning = X.tuning, y.tuning = y.tuning,
x.test = X.test, y.test = y.test,
nlambda = 20, nalpha = 10, pp = pp
)
# Fast DISCOM
results$fast_discom <- fast_discom(
beta = beta.true, x = X.train, y = y.train,
x.tuning = X.tuning, y.tuning = y.tuning,
x.test = X.test, y.test = y.test,
nlambda = 20, pp = pp
)
# Compare performance
comparison <- data.frame(
Method = names(results),
Test_Error = sapply(results, function(x) x$test.error),
R_Squared = sapply(results, function(x) x$R2),
Selected_Vars = sapply(results, function(x) x$select),
Est_Error = sapply(results, function(x) x$est.error),
FPR = sapply(results, function(x) x$fpr),
FNR = sapply(results, function(x) x$fnr),
Time = sapply(results, function(x) x$time)
)
print(comparison)
# Run with robust estimation
result_robust <- adapdiscom(
beta = beta.true,
x = X.train, y = y.train,
x.tuning = X.tuning, y.tuning = y.tuning,
x.test = X.test, y.test = y.test,
nlambda = 20, nalpha = 10, pp = pp,
robust = 1,        # Enable robust estimation
k.value = 1.5      # Huber tuning parameter
)
usethis::use_vignette()
usethis::use_vignette("Simple Usage")
usethis::use_vignette("Simple_Usage")
usethis::use_vignette("AdapDiscom")
install.packages("rhub")
rhub::check_for_cran()
library(rhub)
rhubv2
rhub::rhub_check()
rhub_setup()
rhub_check()
rhub::rhub_doctor()
# Étape 1 : Installation et chargement des packages
# Si vous ne l'avez pas, installez-le d'abord : install.packages("glmmLasso")
library(glmmLasso)
# Étape 2 : Simulation de données
# Nous créons un jeu de données qui imite une cohorte familiale.
set.seed(123) # Pour la reproductibilité
# Définir la structure des données
n_families <- 50      # 50 familles
n_members <- 4         # 4 membres par famille
n_individuals <- n_families * n_members
n_snps <- 100          # 100 SNPs à tester
Re(2)
X = matrix(rnorm(10*10),10,10)
X[5,6] = NA
Xx
X
X[5,] = NA
X[is.na(X)] <- colMeans(X, na.rm = TRUE)[col(X)[is.na(X)]]
X
X = matrix(rnorm(10*10),10,10)
X[8,] = NA
X[is.na(X)] <- colMeans(X, na.rm = TRUE)[col(X)[is.na(X)]]
X
X[5,] = NA
X[8,] = NA
colMeans(X, na.rm = TRUE)
X
X[is.na(X)] <- colMeans(X, na.rm = TRUE)[col(X)[is.na(X)]]
X
XNA = X
XNA[5,] = NA
crossprod(X,XNA)
?crossprod(X,XNA)
N = ifelse(is.na(XNA),0,1)
N
crossprod(N,N)
scale(XNA, center = TRUE, scale = FALSE)
XNA
scale(XNA, center = TRUE, scale = FALSE)
scale(XNA, center = TRUE, scale = FALSE) |> as.matrix()
alpha = rep(1,4)
if(sum(alpha==1)=4)
if(sum(alpha==1)==4)
)
if(sum(alpha==1)==4)
{}
if(sum(alpha==1)==4) print(alpha)
length(NULL)
devtools::submit_cran()
usethis::use_cran_comments()
devtools::submit_cran()
devtools::submit_cran()
devtools::submit_cran()
devtools::submit_cran()
