% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fast_variants.R
\name{fast_adapdiscom}
\alias{fast_adapdiscom}
\title{Fast AdapDiscom}
\usage{
fast_adapdiscom(
  beta,
  x,
  y,
  x.tuning,
  y.tuning,
  x.test,
  y.test,
  nlambda,
  nalpha,
  pp,
  robust = 0,
  n.l = 30,
  standardize = TRUE,
  itcp = TRUE,
  lambda.min.ratio = NULL,
  k.value = 1.5
)
}
\arguments{
\item{beta}{Vector, true beta coefficients (optional)}

\item{x}{Matrix, training data}

\item{y}{Vector, training response}

\item{x.tuning}{Matrix, tuning data}

\item{y.tuning}{Vector, tuning response}

\item{x.test}{Matrix, test data}

\item{y.test}{Vector, test response}

\item{nlambda}{Integer, number of lambda values}

\item{nalpha}{Integer, number of alpha values}

\item{pp}{Vector, block sizes}

\item{robust}{Integer, 0 for classical, 1 for robust estimation}

\item{n.l}{Integer, number of tuning parameter (`l`) values for fast variants}

\item{standardize}{Logical, whether to standardize covariates. When TRUE, uses training data mean and standard deviation to standardize tuning and test sets. When robust=1, uses Huber-robust standard deviation estimates}

\item{itcp}{Logical, whether to include intercept}

\item{lambda.min.ratio}{Numeric, `lambda.min.ratio` sets the smallest lambda value in the grid, expressed as a fraction of `lambda.max`â€”the smallest lambda for which all coefficients are zero. By default, it is `0.0001` when the number of observations (`nobs`) exceeds the number of variables (`nvars`), and `0.01` when `nobs < nvars`. Using a very small value in the latter case can lead to overfitting.}

\item{k.value}{Numeric, tuning parameter for robust estimation}
}
\value{
List with estimation results
}
\description{
Fast AdapDiscom
}
\section{Value}{

The function returns a list containing the following components:

\describe{
  \item{err}{A multi-dimensional array storing the mean squared error (MSE) for all combinations of tuning parameters alpha and lambda.}
  \item{est.error}{The estimation error, calculated as the Euclidean distance between the estimated beta coefficients and the true beta (if provided).}
  \item{lambda}{The optimal lambda value chosen via cross-validation on the tuning set.}
  \item{alpha}{A vector of the optimal alpha values, also selected on the tuning set.}
  \item{train.error}{The mean squared error on the tuning set for the optimal parameter combination.}
  \item{test.error}{The mean squared error on the test set for the final, optimal model.}
  \item{y.pred}{The predicted values for the observations in the test set.}
  \item{R2}{The R-squared value, which measures the proportion of variance explained by the model on the test set.}
  \item{a0}{The intercept of the final model.}
  \item{a1}{The vector of estimated beta coefficients for the final model.}
  \item{select}{The number of non-zero coefficients, representing the number of selected variables.}
  \item{xtx}{The final regularized covariance matrix used to fit the optimal model.}
  \item{fpr}{The False Positive Rate (FPR) if the true beta is provided. It measures the proportion of irrelevant variables incorrectly selected.}
  \item{fnr}{The False Negative Rate (FNR) if the true beta is provided. It measures the proportion of relevant variables incorrectly excluded.}
  \item{lambda.all}{The complete vector of all lambda values tested during cross-validation.}
  \item{beta.cov.lambda.max}{The estimated beta coefficients using the maximum lambda value.}
  \item{time}{The total execution time of the function in seconds.}
}
}

\examples{
\dontrun{
# Fast AdapDiscom example with multiple blocks
set.seed(123)
n <- 150
pp <- c(30, 40, 30)  # Three blocks
p <- sum(pp)
beta_true <- c(rep(0.3, 15), rep(0, 15), rep(-0.2, 10), rep(0, 30), rep(0.4, 20), rep(0, 10))

# Generate covariance matrix
Sigma <- generate.cov(p = p, example = 1)

# Simulate data with complex missing pattern
x_full <- MASS::mvrnorm(n = n, mu = rep(0, p), Sigma = Sigma)
x <- x_full
# Missing pattern: different blocks missing for different observations
miss1 <- sample(1:n, size = floor(n/4))
miss2 <- sample(1:n, size = floor(n/3))
x[miss1, 1:pp[1]] <- NA
x[miss2, (pp[1] + pp[2] + 1):p] <- NA

# Generate response
y <- x_full \%*\% beta_true + rnorm(n, 0, 0.15)

# Split data
train_idx <- 1:90
tune_idx <- 91:120
test_idx <- 121:150

result <- fast_adapdiscom(
  beta = beta_true,
  x = x[train_idx, ],
  y = y[train_idx],
  x.tuning = x[tune_idx, ],
  y.tuning = y[tune_idx],
  x.test = x[test_idx, ],
  y.test = y[test_idx],
  nlambda = 15,
  nalpha = 10,
  pp = pp,
  n.l = 20
)

# View results
print(paste("Test error:", round(result$test.error, 4)))
print(paste("Computation time:", round(result$time, 2), "seconds"))
}
}
